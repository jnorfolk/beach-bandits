# How to use
Download the files updated_beaches.csv and model.py into the same folder. Running model.py will import this file and perform the necessary preprocessing and calculations. It will ask the user to input the name of a starting beach in Florida and the number of nearby beaches the user wishes to visit. Currently, the model is set to find the most optimal route between these nearby beaches, but no more than 9 locations can be used - otherwise it is too "computationally expensive" (will take forever to run). The model will output in JSON the best route, the distance in miles between each location, the total distance, and a map of Florida with a tag on each beach and a line drawn between tags.

The data was acquired from the website https://koordinates.com/layer/111589-florida-beach-names/.

The dataset was constructed by reports from several users including beaches names, their length and the county they were located.

Since the data from the users is missing and we dont really need it we dropped those columns and also dropped the column for the area since we are only interested in the beaches locations, not their size.

For the preprocessing step, we created the columns for latitude and longitude and filled those with the aid of the Open Cage API. After we combined that into a single category called coordinates to improve readability. We later spotted some misspelling in some beaches names and dropped them by hand and dropped as well the duplicates in the dataframe since either the same beach was entered several times by the users. 

By the end we save our preprocessed data into a csv to read when exploring the models, that way we wouldnt have to go to all the process several times since it takes about 4mn to preprocess the data with the API.

The program relies on several functions. The first function calculates and returns the nearest n beaches (using a geodesic distance calculator). A subsequent function calculates a distance matrix, which is a list of lists, each list containing the distance between that location and every other location in the dataframe of nearest n beaches. This function uses scipy.spatial tools to easily calculate and format these values. We then have a branching-off of three separate functions: a random route finder, a nearest neighbors (“best value”) route optimizer, and an ultimate route optimization function (loses performance after 9 locations). The random route finder is randomized, picking a new location regardless of distance. Our nearest neighbors model could be used for hundreds of locations and retain fairly high performance – using just default Python, it finds the nearest location at each junction, and determines a pretty good route. Finally, a brute force model is used as our flagship model, also using default Python, and this finds the best route for a small number of locations. I experimented with using a Google Distance Matrix API and Route Display to incorporate real-time traffic data and navigation tools, but abandoned this approach in favor of the more algorithmic approach when I ran into obstacles with my API requests getting denied. 

One master visualization is used, which projects points and lines onto a geographically-accurate map of Florida. The map is interactive (you could actually see the whole world in detail), and clicking on the location pins will display information about the location (name, coordinates). The lines connecting locations are straight (technically, geodesic, which means they are basically straight but strictly adhere to Earth’s curvature), and provide estimates of travel distance rather than exact values based on traffic.


Conclusions:
The dataset could be further improved by adding more beaches to it. Due to poor data quality, we had to drop some cases that could have improved the model. Also, by including more variables such as beach popularity, amenities, and environmental conditions, we could provide a more detailed and useful resource for picking the next stop on our day. Future work should focus on gathering higher quality data and expanding the dataset to cover more beaches and additional relevant information.
